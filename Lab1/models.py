import math
from typing import Union, Collection, TypeVar

import tensorflow as tf

TFData = TypeVar('TFData')
def f(x: TFData) -> Union[tf.Tensor, tf.Variable, float]:
    if x:
        return "x"


class GMModel:
    def __init__(self, K):
        self.K = K
        self.mean = tf.Variable(tf.random.normal(shape=[K]))
        self.logvar = tf.Variable(tf.random.normal(shape=[K]))
        self.logpi = tf.Variable(tf.zeros(shape=[K]))
        self.var = self.inverse_log_var()
        self.pi = self.inverse_log_pi()
    
    # ðˆ^2=ðžð±ð©(ð¥ð¨ð (ðˆ^2))
    def inverse_log_var(self) -> TFData:
        return tf.math.exp(self.logvar)
    
    # ð‘ðœ½(ð‘§ð‘˜) = ðœ‹ð‘˜ = ð¬ð¨ðŸð­ð¦ðšð±(ð¥ð¨ð ðœ‹ð‘˜)
    def inverse_log_pi(self) -> TFData:
        return tf.nn.softmax(self.logpi)
    
    def update_inverses(self):
        self.var = self.inverse_log_var()
        self.pi = self.inverse_log_pi()
        
    @property
    def variables(self) -> Collection[TFData]:
        return self.mean, self.logvar, self.logpi

    @staticmethod
    def neglog_normal_pdf(x: TFData, mean: TFData, logvar: TFData):
        var = tf.exp(logvar)
        return 0.5 * (tf.math.log(2 * math.pi) + logvar + (x - mean) ** 2 / var)
    
    def normal_pdf(self, x: TFData, mean: TFData, var: TFData) -> TFData:
        return tf.math.exp(-0.5 * (x - mean) ** 2 / var) / tf.math.sqrt(2 * math.pi * var)

    @tf.function
    def loss(self, data: TFData):
        return self.loss_x(data)
    
    # ð¿ðœ½(ð‘¥^(ð‘–)|ð‘§ð‘˜) = 0.5 â‹… (log(2ðœ‹) + log(ðœŽ^2) + (ð‘¥âˆ’ðœ‡)^2/ðœŽ^2) 
    def loss_xz(self, x: TFData, k: int):
        return self.neglog_normal_pdf(x, self.mean[k], self.logvar[k])
    
    # ð¿ðœ½(ð‘§ð‘˜) = logâˆ‘exp(ð¥ð¨ð ðœ‹ð‘—)âˆ’(logðœ‹ð‘˜)
    def loss_z(self, k: int):
        return tf.reduce_logsumexp(self.logpi) - self.logpi[k]
    
    # ð¿ðœ½(ð‘¥(ð‘–)) = âˆ’logâˆ‘exp(âˆ’(ð¿ðœ½(ð‘¥^(ð‘–)|ð‘§ð‘˜) + ð¿ðœ½(ð‘§ð‘˜)))
    def loss_x(self, x: TFData):
        exp_arg = [(-1) * (self.loss_xz(x, k) + self.loss_z(k)) for k in range(self.K)]
        return (-1) * tf.reduce_logsumexp(exp_arg, axis=0) # Axis == 0 da ocuvamo (1000,1) jer smo vec u exp_arg sumirali po k
    
    def p_z(self, k: int) -> TFData:
        return self.pi[k]
    
    def p_xz(self, x: TFData, k: int) -> TFData:
        return self.normal_pdf(x, self.mean[k], self.var[k])

    def p_x(self, x: TFData) -> TFData:
        return tf.math.reduce_sum([self.pi[k] * self.p_xz(x, k) for k in range(self.K)], axis=0)
        
    
